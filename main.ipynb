{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3533,
   "id": "6c89cfe1-6927-44c6-8e2d-373f3f5bcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "R\n",
      "N\n",
      "R\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "R\n",
      "N\n",
      "R\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "R\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "R\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('Cancer_dataset1.csv')\n",
    "\n",
    "for i in df.outcome:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3534,
   "id": "8678835d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN RADIUS\n",
      "Mean Radius Count = 194\n",
      "Mean Radius Mean = 17.412061855670103\n",
      "Mean Radius Standard Deviation = 3.1571814471858555\n",
      "Mean Radius Minimum = 10.95\n",
      "Mean Radius 25% percentile = 15.0525\n",
      "Mean Radius 50% percentile = 17.28\n",
      "Mean Radius 75% percentile = 19.58\n",
      "Mean Radius Maximum = 27.22\n",
      "\n",
      "\n",
      "MEAN TEXTURE\n",
      "Mean Texture Count = 194\n",
      "Mean Texture Mean = 22.3190206185567\n",
      "Mean Texture Standard Deviation = 4.283068191040899\n",
      "Mean Texture Minimum = 10.38\n",
      "Mean Texture 25% percentile = 19.517500000000002\n",
      "Mean Texture 50% percentile = 21.795\n",
      "Mean Texture 75% percentile = 24.655\n",
      "Mean Texture Maximum = 39.28\n",
      "\n",
      "\n",
      "MEAN PERIMETER\n",
      "Mean Perimeter Count = 198\n",
      "Mean Perimeter Mean = 114.85656565656565\n",
      "Mean Perimeter Standard Deviation = 21.383401559552883\n",
      "Mean Perimeter Minimum = 71.9\n",
      "Mean Perimeter 25% percentile = 98.16\n",
      "Mean Perimeter 50% percentile = 113.7\n",
      "Mean Perimeter 75% percentile = 129.64999999999998\n",
      "Mean Perimeter Maximum = 182.1\n",
      "\n",
      "\n",
      "MEAN AREA\n",
      "Mean Area Count = 198\n",
      "Mean Area Mean = 970.0409090909089\n",
      "Mean Area Standard Deviation = 352.14921516208284\n",
      "Mean Area Minimum = 361.6\n",
      "Mean Area 25% percentile = 702.525\n",
      "Mean Area 50% percentile = 929.0999999999999\n",
      "Mean Area 75% percentile = 1193.5\n",
      "Mean Area Maximum = 2250.0\n",
      "\n",
      "\n",
      "MEAN SMOOTHNESS\n",
      "Mean Smoothness Count = 198\n",
      "Mean Smoothness Mean = 0.10268141414141414\n",
      "Mean Smoothness Standard Deviation = 0.012522431569239916\n",
      "Mean Smoothness Minimum = 0.07497\n",
      "Mean Smoothness 25% percentile = 0.0939\n",
      "Mean Smoothness 50% percentile = 0.10189999999999999\n",
      "Mean Smoothness 75% percentile = 0.110975\n",
      "Mean Smoothness Maximum = 0.1447\n",
      "\n",
      "\n",
      "MEAN COMPACTNESS\n",
      "Mean Compactness Count = 198\n",
      "Mean Compactness Mean = 0.14264777777777776\n",
      "Mean Compactness Standard Deviation = 0.049897601888512416\n",
      "Mean Compactness Minimum = 0.04605\n",
      "Mean Compactness 25% percentile = 0.11019999999999999\n",
      "Mean Compactness 50% percentile = 0.13175\n",
      "Mean Compactness 75% percentile = 0.17220000000000002\n",
      "Mean Compactness Maximum = 0.3114\n",
      "\n",
      "\n",
      "MEAN CONCAVITY\n",
      "Mean Concavity Count = 198\n",
      "Mean Concavity Mean = 0.15624277777777776\n",
      "Mean Concavity Standard Deviation = 0.07057226120534643\n",
      "Mean Concavity Minimum = 0.02398\n",
      "Mean Concavity 25% percentile = 0.10685\n",
      "Mean Concavity 50% percentile = 0.15134999999999998\n",
      "Mean Concavity 75% percentile = 0.2005\n",
      "Mean Concavity Maximum = 0.4268\n",
      "\n",
      "\n",
      "MEAN CONCAVE POINTS\n",
      "Mean Concave Points Count = 198\n",
      "Mean Concave Points Mean = 0.08677560606060604\n",
      "Mean Concave Points Standard Deviation = 0.03387663129061639\n",
      "Mean Concave Points Minimum = 0.02031\n",
      "Mean Concave Points 25% percentile = 0.06367\n",
      "Mean Concave Points 50% percentile = 0.086075\n",
      "Mean Concave Points 75% percentile = 0.103925\n",
      "Mean Concave Points Maximum = 0.2012\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1A)\n",
    "\n",
    "#Summary statistics for mean_radius\n",
    "print(\"MEAN RADIUS\")\n",
    "print(f\"Mean Radius Count = {df['mean_radius'].count()}\")\n",
    "print(f\"Mean Radius Mean = {df['mean_radius'].mean()}\")\n",
    "print(f\"Mean Radius Standard Deviation = {df['mean_radius'].std()}\")\n",
    "print(f\"Mean Radius Minimum = {df['mean_radius'].min()}\")\n",
    "print(f\"Mean Radius 25% percentile = {df['mean_radius'].quantile(0.25)}\")\n",
    "print(f\"Mean Radius 50% percentile = {df['mean_radius'].quantile(0.50)}\")\n",
    "print(f\"Mean Radius 75% percentile = {df['mean_radius'].quantile(0.75)}\")\n",
    "print(f\"Mean Radius Maximum = {df['mean_radius'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"MEAN TEXTURE\")\n",
    "print(f\"Mean Texture Count = {df['mean_texture'].count()}\")\n",
    "print(f\"Mean Texture Mean = {df['mean_texture'].mean()}\")\n",
    "print(f\"Mean Texture Standard Deviation = {df['mean_texture'].std()}\")\n",
    "print(f\"Mean Texture Minimum = {df['mean_texture'].min()}\")\n",
    "print(f\"Mean Texture 25% percentile = {df['mean_texture'].quantile(0.25)}\")\n",
    "print(f\"Mean Texture 50% percentile = {df['mean_texture'].quantile(0.50)}\")\n",
    "print(f\"Mean Texture 75% percentile = {df['mean_texture'].quantile(0.75)}\")\n",
    "print(f\"Mean Texture Maximum = {df['mean_texture'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"MEAN PERIMETER\")\n",
    "print(f\"Mean Perimeter Count = {df['mean_perimeter'].count()}\")\n",
    "print(f\"Mean Perimeter Mean = {df['mean_perimeter'].mean()}\")\n",
    "print(f\"Mean Perimeter Standard Deviation = {df['mean_perimeter'].std()}\")\n",
    "print(f\"Mean Perimeter Minimum = {df['mean_perimeter'].min()}\")\n",
    "print(f\"Mean Perimeter 25% percentile = {df['mean_perimeter'].quantile(0.25)}\")\n",
    "print(f\"Mean Perimeter 50% percentile = {df['mean_perimeter'].quantile(0.50)}\")\n",
    "print(f\"Mean Perimeter 75% percentile = {df['mean_perimeter'].quantile(0.75)}\")\n",
    "print(f\"Mean Perimeter Maximum = {df['mean_perimeter'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"MEAN AREA\")\n",
    "print(f\"Mean Area Count = {df['mean_area'].count()}\")\n",
    "print(f\"Mean Area Mean = {df['mean_area'].mean()}\")\n",
    "print(f\"Mean Area Standard Deviation = {df['mean_area'].std()}\")\n",
    "print(f\"Mean Area Minimum = {df['mean_area'].min()}\")\n",
    "print(f\"Mean Area 25% percentile = {df['mean_area'].quantile(0.25)}\")\n",
    "print(f\"Mean Area 50% percentile = {df['mean_area'].quantile(0.50)}\")\n",
    "print(f\"Mean Area 75% percentile = {df['mean_area'].quantile(0.75)}\")\n",
    "print(f\"Mean Area Maximum = {df['mean_area'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"MEAN SMOOTHNESS\")\n",
    "print(f\"Mean Smoothness Count = {df['mean_smoothness'].count()}\")\n",
    "print(f\"Mean Smoothness Mean = {df['mean_smoothness'].mean()}\")\n",
    "print(f\"Mean Smoothness Standard Deviation = {df['mean_smoothness'].std()}\")\n",
    "print(f\"Mean Smoothness Minimum = {df['mean_smoothness'].min()}\")\n",
    "print(f\"Mean Smoothness 25% percentile = {df['mean_smoothness'].quantile(0.25)}\")\n",
    "print(f\"Mean Smoothness 50% percentile = {df['mean_smoothness'].quantile(0.50)}\")\n",
    "print(f\"Mean Smoothness 75% percentile = {df['mean_smoothness'].quantile(0.75)}\")\n",
    "print(f\"Mean Smoothness Maximum = {df['mean_smoothness'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"MEAN COMPACTNESS\")\n",
    "print(f\"Mean Compactness Count = {df['mean_compactness'].count()}\")\n",
    "print(f\"Mean Compactness Mean = {df['mean_compactness'].mean()}\")\n",
    "print(f\"Mean Compactness Standard Deviation = {df['mean_compactness'].std()}\")\n",
    "print(f\"Mean Compactness Minimum = {df['mean_compactness'].min()}\")\n",
    "print(f\"Mean Compactness 25% percentile = {df['mean_compactness'].quantile(0.25)}\")\n",
    "print(f\"Mean Compactness 50% percentile = {df['mean_compactness'].quantile(0.50)}\")\n",
    "print(f\"Mean Compactness 75% percentile = {df['mean_compactness'].quantile(0.75)}\")\n",
    "print(f\"Mean Compactness Maximum = {df['mean_compactness'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"MEAN CONCAVITY\")\n",
    "print(f\"Mean Concavity Count = {df['mean_concavity'].count()}\")\n",
    "print(f\"Mean Concavity Mean = {df['mean_concavity'].mean()}\")\n",
    "print(f\"Mean Concavity Standard Deviation = {df['mean_concavity'].std()}\")\n",
    "print(f\"Mean Concavity Minimum = {df['mean_concavity'].min()}\")\n",
    "print(f\"Mean Concavity 25% percentile = {df['mean_concavity'].quantile(0.25)}\")\n",
    "print(f\"Mean Concavity 50% percentile = {df['mean_concavity'].quantile(0.50)}\")\n",
    "print(f\"Mean Concavity 75% percentile = {df['mean_concavity'].quantile(0.75)}\")\n",
    "print(f\"Mean Concavity Maximum = {df['mean_concavity'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"MEAN CONCAVE POINTS\")\n",
    "print(f\"Mean Concave Points Count = {df['mean_concave_points'].count()}\")\n",
    "print(f\"Mean Concave Points Mean = {df['mean_concave_points'].mean()}\")\n",
    "print(f\"Mean Concave Points Standard Deviation = {df['mean_concave_points'].std()}\")\n",
    "print(f\"Mean Concave Points Minimum = {df['mean_concave_points'].min()}\")\n",
    "print(f\"Mean Concave Points 25% percentile = {df['mean_concave_points'].quantile(0.25)}\")\n",
    "print(f\"Mean Concave Points 50% percentile = {df['mean_concave_points'].quantile(0.50)}\")\n",
    "print(f\"Mean Concave Points 75% percentile = {df['mean_concave_points'].quantile(0.75)}\")\n",
    "print(f\"Mean Concave Points Maximum = {df['mean_concave_points'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3535,
   "id": "3879b861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTCOME\n",
      "Outcome Count = 198\n",
      "Outcome unique values are = ['N' 'R']\n",
      "Count of each unique values are = N    151\n",
      "R     47\n",
      "Name: outcome, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 1B)\n",
    "\n",
    "print(\"OUTCOME\")\n",
    "print(f\"Outcome Count = {df['outcome'].count()}\")\n",
    "print(f\"Outcome unique values are = {df['outcome'].unique()}\")\n",
    "print(f\"Count of each unique values are = {df['outcome'].value_counts()}\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3536,
   "id": "9dfb77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1C)\n",
    "df['outcome'].replace('N', 0, inplace=True) \n",
    "df['outcome'].replace('R', 1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3537,
   "id": "2e1e8a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mean_perimeter  se_perimeter\n",
      "mean_perimeter        1.000000      0.609964\n",
      "se_perimeter          0.609964      1.000000\n"
     ]
    }
   ],
   "source": [
    "# Question 1E)\n",
    "print(df[['mean_perimeter','se_perimeter']].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df45305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Class for cleaning up data and data manipulation in general\n",
    "class DataManipulation:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def cleanUpData(self):\n",
    "        self.df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    def divideData(self):\n",
    "\n",
    "        # Using stratification to balance the data in the training and the test data set.\n",
    "        \n",
    "        df_negative = self.df[self.df['outcome'] == 0]\n",
    "        df_positive = self.df[self.df['outcome'] == 1]\n",
    "        \n",
    "        \n",
    "        print(len(df_negative))\n",
    "        print(len(df_positive))\n",
    "\n",
    "        df_negative_train = df_negative.sample(n=50) # taking 50 samples from negative class for training for stratification\n",
    "        df_positive_train = df_positive.sample(n=35) # taking 35 samples from the positive class for training for stratification\n",
    "        \n",
    "\n",
    "        df_negative_test =  df_negative[~df_negative.apply(tuple, axis=1).isin(df_negative_train.apply(tuple, axis=1))]\n",
    "        df_positive_test =  df_positive[~df_positive.apply(tuple, axis=1).isin(df_positive_train.apply(tuple, axis=1))]\n",
    "\n",
    "        df_negative_test_final = df_negative_test.sample(n=8) # taking 8 samples from the positive class for test for stratification \n",
    "        \n",
    "        train = pd.concat([df_positive_train, df_negative_train], ignore_index=True)\n",
    "        test = pd.concat([df_positive_test, df_negative_test_final], ignore_index=True)\n",
    "\n",
    "        train_shuffled = train.sample(frac=1).reset_index(drop=True)\n",
    "        test_shuffled = test.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        return train_shuffled, test_shuffled\n",
    "\n",
    "    \n",
    "    def reqColsFromData(self, cols, data : pd.DataFrame):\n",
    "        finalData = []\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            predictorsArr = []\n",
    "\n",
    "            for col in cols:\n",
    "                predictorsArr.append(row[col])\n",
    "        \n",
    "            finalData.append((predictorsArr, row['outcome']))\n",
    "            \n",
    "        \n",
    "        return finalData\n",
    "    \n",
    "    def performZScoreScaling(self, data):\n",
    "\n",
    "        for x in data:\n",
    "            for index, _ in enumerate(x[0]):\n",
    "                MEAN, STD = self.getMeanStd(index, data)\n",
    "                x[0][index] = (Decimal(x[0][index]) - MEAN) / STD\n",
    "\n",
    "        return data\n",
    "\n",
    "    def getMeanStd(self, col, data):    \n",
    "        dataOfCol = []\n",
    "\n",
    "        for x in data:\n",
    "            dataOfCol.append(Decimal(x[0][col]))\n",
    "\n",
    "        mean = np.mean(dataOfCol)\n",
    "        std = np.std(dataOfCol)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Logistic regression model Class with all the components required for Logistic regression\n",
    "class LogisticRegression:\n",
    "    \n",
    "    trainingData = None\n",
    "    testData = None\n",
    "    maxNoOfIterations = 1000\n",
    "\n",
    "    def __init__(self, trainingData, testData, thetas, lr, isRegularization):\n",
    "        self.trainingData = trainingData\n",
    "        self.testData = testData\n",
    "        self.thetas = thetas\n",
    "        self.lr = lr\n",
    "        self.isRegularization = isRegularization\n",
    "        \n",
    "        \n",
    "    def getFeatures2D(self, data):\n",
    "        if data != None:\n",
    "            features = [x[0] for x in data]\n",
    "            return features\n",
    "\n",
    "    def getOutputs(self, data):\n",
    "        if data != None:\n",
    "            output = [Decimal(x[1]) for x in data]\n",
    "            return output\n",
    "\n",
    "    def performLogisticRegression(self):\n",
    "        self.iterate()\n",
    "\n",
    "    def computePredictedOutput(self, thetas, data):\n",
    "        predictedOutputs = []\n",
    "\n",
    "        for x in self.getFeatures2D(data):\n",
    "            y = Decimal(thetas[0])\n",
    "\n",
    "            for idx, i in enumerate(x):\n",
    "                y += (Decimal(thetas[idx + 1]) * Decimal(i))\n",
    "\n",
    "            \n",
    "            Y = Decimal(1 / (1 + np.exp(-y) ))\n",
    "            \n",
    "            predictedOutputs.append(Y)\n",
    "\n",
    "        return predictedOutputs\n",
    "    \n",
    "\n",
    "    def calculateGradientDescent(self, B, index,  predictedOutputs, isIntercept):\n",
    "        features = self.getFeatures2D(self.trainingData)\n",
    "        outputs = self.getOutputs(self.trainingData)\n",
    "\n",
    "        dataLen = Decimal(len(features))\n",
    "        \n",
    "        total = 0\n",
    "\n",
    "        for idx, x in enumerate(features):\n",
    "\n",
    "            if isIntercept:\n",
    "                result = (predictedOutputs[idx] - outputs[idx])\n",
    "            else:\n",
    "                result = (predictedOutputs[idx] - outputs[idx]) * Decimal(x[index]) \n",
    "\n",
    "\n",
    "            total += result\n",
    "\n",
    "        if self.isRegularization and (not isIntercept):\n",
    "            total += (100 * B) / dataLen # regularization coefficent = 100\n",
    "        \n",
    "        change = (Decimal(self.lr) * total) / dataLen\n",
    "\n",
    "        newB = Decimal(B) - change\n",
    "\n",
    "        \n",
    "        return newB\n",
    "    \n",
    "    def iterate(self):\n",
    "        \n",
    "\n",
    "        for i in range(self.maxNoOfIterations):\n",
    "\n",
    "            predictedOutputs = self.computePredictedOutput(self.thetas, self.trainingData)\n",
    "\n",
    "            for idx, b in enumerate(self.thetas):\n",
    "                if idx == 0:\n",
    "                    newB = self.calculateGradientDescent(b, (idx - 1), predictedOutputs, True)\n",
    "                else:\n",
    "                    newB = self.calculateGradientDescent(b, (idx - 1), predictedOutputs, False)\n",
    "\n",
    "                self.thetas[idx] = newB\n",
    "\n",
    "        \n",
    "        self.printResults()\n",
    "\n",
    "    def calculateCost(self, predictedOutputs, data):\n",
    "        outputs = self.getOutputs(data)\n",
    "        dataLen = len(self.getFeatures2D(data))\n",
    "\n",
    "        total = 0\n",
    "\n",
    "        for idx, x in enumerate(predictedOutputs):\n",
    "            result = (outputs[idx] * x.log10()) + ((1 - outputs[idx]) * (1 - x).log10())\n",
    "            total += result\n",
    "\n",
    "        return -total/(dataLen)\n",
    "\n",
    "    \n",
    "    def calculateF1Score(self, predictedOutputs, data, threshold):\n",
    "        outputs = self.getOutputs(data)\n",
    "       \n",
    "        roundedPredictedOutputs = []\n",
    "\n",
    "        for idx, x in enumerate(predictedOutputs):\n",
    "                \n",
    "            if x > threshold:  \n",
    "                roundedPredictedOutputs.append(1)\n",
    "            else:\n",
    "                roundedPredictedOutputs.append(0)\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "\n",
    "        print(f\"The \")\n",
    "\n",
    "        for idx, x in enumerate(roundedPredictedOutputs):\n",
    "            if outputs[idx] == 1 and x == 1:\n",
    "                TP += 1\n",
    "            elif outputs[idx] == 1 and x == 0:\n",
    "                FN += 1\n",
    "            elif outputs[idx] == 0 and x == 1:\n",
    "                FP += 1\n",
    "\n",
    "        # print(f\"true positive: {TP}\")\n",
    "        # print(f\"false negative: {FN}\")\n",
    "        # print(f\"false positive: {FP}\")\n",
    "\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "\n",
    "        if TP + FP == 0 :\n",
    "            pass\n",
    "        else:\n",
    "            precision = TP/(TP + FP)\n",
    "\n",
    "        if TP + FN == 0:\n",
    "            pass\n",
    "        else:\n",
    "            recall = TP/(TP + FN)\n",
    "\n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return ( 2 * (precision * recall) / (precision + recall))\n",
    "            \n",
    "\n",
    "        return f1\n",
    "\n",
    "    def printResults(self):\n",
    "        print(\"The values of the Thetas/ Weights are\")\n",
    "        print(self.thetas)\n",
    "        \n",
    "        predictedOutputs = self.computePredictedOutput(self.thetas, self.testData)\n",
    "\n",
    "        cost = self.calculateCost(predictedOutputs, self.testData)\n",
    "        print(f\"Cost of the Test data is: {cost}\")\n",
    "\n",
    "        \n",
    "        f1score = self.calculateF1Score(predictedOutputs, self.testData, 0.45)\n",
    "        print(f\"The f1score is : {f1score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3539,
   "id": "5ccbd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# Getting data ready for logistic regression\n",
    "\n",
    "dm = DataManipulation(df)\n",
    "dm.cleanUpData()\n",
    "\n",
    "train, test = dm.divideData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3540,
   "id": "33e9abfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of the Thetas/ Weights are\n",
      "[Decimal('-4.368102192596630329232767816'), Decimal('0.2990470197906772179096476186')]\n",
      "Cost of the Test data is: 0.01374084880361386823760637468\n",
      "The f1score is : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This is not in our assignment. Just checking if the actual Logistic regression code works or not\n",
    "finalTrain = [([1], 0), ([2], 0), ([3], 0), ([4], 0), ([30], 1), ([40], 1), ([50], 1), ([60], 1)]\n",
    "\n",
    "finalTest = [([70], 1), ([5], 0), ([6], 0), ([2.5], 0), ([0], 0), ([1.3], 0), ([3.5], 0)]\n",
    "\n",
    "lr = LogisticRegression(finalTrain, finalTest, [0.1, 0.2], 0.1, False) # passing in Training data, Test data, max no of iteration, starting theta values,\n",
    "                                                                  # the learning date and regularization\n",
    "lr.performLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3541,
   "id": "ff0d2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of the Thetas/ Weights are\n",
      "[Decimal('-0.5553183839738671079474369119'), Decimal('0.1711625176612601121703319314')]\n",
      "Cost of the Test data is: 0.2879355437696036155289133405\n",
      "The f1score is : 0.4\n"
     ]
    }
   ],
   "source": [
    "# Question 2A and 2B\n",
    "tempTrain = dm.reqColsFromData(['mean_area'], train)\n",
    "tempTest = dm.reqColsFromData(['mean_area'], test)\n",
    "\n",
    "finalTrain = dm.performZScoreScaling(tempTrain)\n",
    "finalTest = dm.performZScoreScaling(tempTest)\n",
    "\n",
    "lr = LogisticRegression(finalTrain, finalTest, [0.1, 0.2], 0.1, False) # passing in Training data, Test data, max no of iteration, starting theta values,\n",
    "                                                                  # the learning date and regularization\n",
    "lr.performLogisticRegression()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3542,
   "id": "b87584c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of the Thetas/ Weights are\n",
      "[Decimal('-0.1971406057056817058097281273'), Decimal('-0.3096386434547396329679464212'), Decimal('-0.3400657314775427784165679963'), Decimal('-0.1599714693234976406289731671'), Decimal('0.7287755823994215763859589434'), Decimal('0.2758278813932275003748928777'), Decimal('-0.1824824671600028417916824940'), Decimal('-0.3386403169654677769929882379'), Decimal('-0.1079436024398823559346926476'), Decimal('0.9832335335242528049358490172'), Decimal('-0.1080879120859512144847558179'), Decimal('-0.2222780805158437463580328880'), Decimal('0.2656301064489918928877077657')]\n",
      "Cost of the Test data is: 0.2923558719235205035773676029\n",
      "The f1score is : 0.625\n"
     ]
    }
   ],
   "source": [
    "#Question 3A\n",
    "\n",
    "tempTrain = dm.reqColsFromData(['mean_radius',\n",
    "                                'mean_texture',\n",
    "                                'mean_perimeter',\n",
    "                                'mean_area',\n",
    "                                'mean_smoothness',\n",
    "                                'mean_compactness',\n",
    "                                'mean_concavity',\n",
    "                                'mean_concave_points',\n",
    "                                'mean_fractal_dimension',\n",
    "                                'se_perimeter',\n",
    "                                'se_texture',\n",
    "                                'se_area'], train)\n",
    "tempTest = dm.reqColsFromData(['mean_radius',\n",
    "                                'mean_texture',\n",
    "                                'mean_perimeter',\n",
    "                                'mean_area',\n",
    "                                'mean_smoothness',\n",
    "                                'mean_compactness',\n",
    "                                'mean_concavity',\n",
    "                                'mean_concave_points',\n",
    "                                'mean_fractal_dimension',\n",
    "                                'se_perimeter',\n",
    "                                'se_texture',\n",
    "                                'se_area'], test)\n",
    "\n",
    "\n",
    "finalTrain = dm.performZScoreScaling(tempTrain)\n",
    "finalTest = dm.performZScoreScaling(tempTest)\n",
    "\n",
    "lr = LogisticRegression(finalTrain, finalTest, [0.1, 0.2, 0.1, 0.1, 0.0, 0.2, 0.1, 0.3, 0.0, 0.1, 0.2, 0.3, 0.0], 0.1, False) \n",
    "lr.performLogisticRegression() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b87798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
